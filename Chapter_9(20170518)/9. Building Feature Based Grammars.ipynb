{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(300000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어에는 광범위한 문법 구조가 있으므로, 8장에서 다룬 간단한 방법으로 다루기 어렵다. 유연성을 높이기 위해 S, NP 그리고 V와 같은 문법 요소들의 처리 방법을 달리 할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장의 목적은 다음 질문에 답하는 것이다.\n",
    "\n",
    "1. context-free 문법 프레임워크를 어떻게 확장하여 문법적 분류와 생산을 정교화할 수 있는가?\n",
    "2. 특징 구조(feature structure)의 주요 속성은 무엇이며, 어떻게 computationally 사용할 수 있는가?\n",
    "3. 특징 구조 문법을 통해 이제 어떤 종류의 언어학적 패턴과 문법 구조를 파악할 수 있는가? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Grammatical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6장에서는 텍스트의 특징을 감지하는 것에 의존하는 classifier를 만드는 법을 설명했는데, 이 특징들은 단어의 마지막 글자를 추출하는 것처럼 간단할 수도 있고, 혹은 classifier로부터 예측된 POS처럼 다소 복잡할 수도 있다. 여기서는 rule-based 문법을 구현하는데 있어 특징(feature)들의 역할에 대해 공부해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kim = { 'CAT' : 'NP' , 'ORTH' : 'Kim' , 'REF' : 'k' }\n",
    "chase = { 'CAT' : 'V' , 'ORTH' : 'chased' , 'REL' : 'chase' } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Structures(feature와 value pair)\n",
    " - 'CAT' : 문법적 범주\n",
    " - 'ORTH' : 표기법(철자법)\n",
    " - 'REF'/'REL' : 추가 정보(REFerent, RELation, etc.)\n",
    "\n",
    "위와 같이 feature structure는 문법 개체에 대한 다양한 정보를 포함하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 정보가 모든 정보를 포함할 필요는 없으며, 아래와 같이 속성을 추가할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chase['AGT'] = 'sbj'\n",
    "chase['PAT'] = 'obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"Kim chased Lee\"\n",
    "tokens = sent.split()\n",
    "lee = {'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Kim chased Lee\" 라는 문장에서 \"chase\" 앞뒤 단어를 동사의 Agent(sbj, 주체)와 Patient(obj, 객체)로 엮어서 처리하고 싶다. 이 예제에서는 동사의 왼쪽과 오른쪽에 있는 NP가 각각 주체와 객체라는 단순한 가정을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lex2fs(word):\n",
    "    for fs in [kim, lee, chase]:\n",
    "        if fs['ORTH'] == word:\n",
    "            return fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표기법이 맞아떨어지면 해당 토큰(단어)에 feature structure를 갖다 붙이는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGT': 'sbj', 'CAT': 'V', 'ORTH': 'chased', 'PAT': 'obj', 'REL': 'chase'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예문에서는 agent가 k, patient가 l이므로 verb의 feature structure의 속성 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verb['AGT'] = subj['REF']\n",
    "verb['PAT'] = obj['REF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGT': 'k', 'CAT': 'V', 'ORTH': 'chased', 'PAT': 'l', 'REL': 'chase'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORTH  => chased\n",
      "REL   => chase\n",
      "AGT   => k\n",
      "PAT   => l\n"
     ]
    }
   ],
   "source": [
    "for k in ['ORTH', 'REL', 'AGT', 'PAT']:\n",
    "    print(\"%-5s => %s\" % (k, verb[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature structure는 꽤 강력하지만 위의 예제는 사실 설명을 위해 조작한 것이다. 다음은 context-free 문법 및 구문 분석의 프레임워크가 feature structure를 도입하여 좀 더 일반적인 방식으로 구현하는 방법을 소개할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Syntactic Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 간단하게 문법에 대한 합의를 하고난 후 다음 장으로 넘어간다.\n",
    "\n",
    "수 일치\n",
    " - 단수, 복수\n",
    " - 인칭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 문장을 3인칭 단수, 3인칭 복수 등과 같이 encoding 후 context-free grammar(CFG)를 적용한 모습이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/missing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 (7)에서 우리는 'this dog runs'라는 문장을 생성했다. 그러나 우리는 'these dogs run'과 같은 문법적으로 올바른 다른 문장도 얻고 싶다. 가장 직관적인 방법은 grammar에 새로운 non-terminals와 productions를 추가하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 (7)에 비해 (6)은 단수형, 복수형을 구분해서 갖고있으므로 2배의 정보를 갖고있는 셈이다. 작은 문법에 있어서는 큰 문제는 안되겠지만, 영어 문법의 전반을 다루기 위해서는 그리 아름다운 size는 아니다. 더 좋은 방법은 없을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Using Attributes and Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 속성(properties)이라는 것을 갖는 언어학적 범주에 대해 비공식적으로 얘기했다. 예를 들어 명사는 복수형이라는 속성이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 NUM은 number의 약자로, 수 일치에 활용되는 속성이며, sg(singlular)와 pl(plural)의 값을 가질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__S -> NP[NUM=?n] VP[NUM=?n]__\n",
    "\n",
    "S(sentence)는 NP(noun phrase)와 VP(verb phrase)로 구성되어 있는데 NUM이 sg 혹은 pl이든 구조는 변함이 없다. 다시 NP도 아래와 같이 나뉜다.\n",
    "\n",
    "__NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 (12)의 경우에는 수 일치가 잘 된 모습, (13)의 경우 수 일치에 실패(FAIL)한 모습. \n",
    "depth 1인 dog를 시작으로 앞의 this와 수 일치가 되었는지 확인하고 NP의 NUM 속성 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "짜잔 ~ 복수형으로 수일치가 된 문장의 tree 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this와 these 같은 determiners는 단수냐 복수냐에 대한 정보가 필요하지만, the, some, any 같은 경우에는 명사의 수에 따라 선택하는 것이 아니다.\n",
    "\n",
    "__Det[NUM=?n] -> 'the' | 'some' | 'any'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n] \n",
      "NP[NUM=?n] -> PropN[NUM=?n] \n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl] \n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children' \n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUM 뿐만 아니라 둘 이상의 속성도 가질 수 있다. 예를 들어, 동사의 경우에는 TENSE도 함께 들어가 시제를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = 'Kim likes children'.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/feat0.fcfg', trace=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Kim .like.chil.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'Kim'\n",
      "|.    [----]    .| [1:2] 'likes'\n",
      "|.    .    [----]| [2:3] 'children'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] PropN[NUM='sg'] -> 'Kim' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [---->    .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] N[NUM='pl'] -> 'children' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [---->| [2:3] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|.    [---------]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[==============]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n"
     ]
    }
   ],
   "source": [
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 sg, pl과 같은 feature 값들만 봐왔는데, 이런 단순한 값들을 __atomic__ 이라고 일반적으로 부른다. 즉, 더이상 서브파트로 나뉠 수 없음을 의미한다. atomic의 특수 케이스로 __boolean__ 값이 있는데, 예를 들어 __auxiliary__ 동사(조동사)인지의 여부를 나타내는 feature로 AUX를 사용한다. \n",
    "\n",
    "__V[TENSE=pres, AUX=+] -> 'can'__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 문법 범주(verb, noun, etc.)에 feature annotation을 하는 경우만 봐왔지만, 아예 문법 범주 조차도 POS feature에 추가하여 표기할 수도 있다.(중요하지는 않은듯)\n",
    "\n",
    "AGR : Agreement Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing Feature Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 섹션에서는 NLTK에서 feature structure가 어떻게 구성되고 조정되는지에 대해 알아볼 것이다. 또한, 합성의 방법에 대해서도 논의 하여 서로 다른 두 가지 feature structure에 포함 된 정보를 결합할 수 있게 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ NUM   = 'sg'   ]\n",
      "[ TENSE = 'past' ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(TENSE='past', NUM='sg')\n",
    "print(fs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fem\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(PER=3, NUM='pl', GND='fem')\n",
    "print(fs1['GND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs1['CASE'] = 'acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CASE='acc', GND='fem', NUM='pl', PER=3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agreement feature인 fs1을 포함하는 feature structure, fs2를 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ CASE = 'acc' ] ]\n",
      "[ AGR = [ GND  = 'fem' ] ]\n",
      "[       [ NUM  = 'pl'  ] ]\n",
      "[       [ PER  = 3     ] ]\n",
      "[                        ]\n",
      "[ POS = 'N'              ]\n"
     ]
    }
   ],
   "source": [
    "fs2 = nltk.FeatStruct(POS='N', AGR=fs1)\n",
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CASE = 'acc' ]\n",
      "[ GND  = 'fem' ]\n",
      "[ NUM  = 'pl'  ]\n",
      "[ PER  = 3     ]\n"
     ]
    }
   ],
   "source": [
    "print(fs2['AGR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아예 feature-value 쌍의 string의 형태로 feature structure를 선언할 수도 있다. 누가 쓸 지는 모르겠지만."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ GND = 'fem' ] ]\n",
      "[ AGR = [ NUM = 'pl'  ] ]\n",
      "[       [ PER = 3     ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(\"[POS='N', AGR=[PER=3, NUM='pl', GND='fem']]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature structure가 근본적으로는 언어학적 목적으로만 사용되는 건 아니고, __그래프__라고 여기는 것이 종종 편리하다. 특히, __directed acyclic graphs(DAGS)__와 상당히 닮았다.\n",
    "\n",
    "__자연어 처리와는 무관한 설명이므로 거침없이 생략한다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Subsumption and Unification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature structure는 어떤 특정 객체에 대해 __부분 정보(partial information)__를 제공하는 것으로 여기는 것이 일반적이다. 얼마나 많은 정보를 포함하는 feature structure냐에 따라 순서를 정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(23)의 경우 c가 가장 많은 정보를 포함하고 있으며, b의 경우에는 모든 정보가 c에도 포함되어 있으므로 c의 subsumption이라 할 수 있다.\n",
    "\n",
    "어느 하나에 포함되는 feature structure의 관계가 아니라 필요한 정보를 덧붙이는 경우에는 어떨까? 주소에는 street 이름과 number 뿐만 아니라 city에 대한 정보가 필요한데, 이때 a와 b를 merge하여 c를 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/25.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(NUMBER=74, STREET='rue Pascal')\n",
    "fs2 = nltk.FeatStruct(CITY='Paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n"
     ]
    }
   ],
   "source": [
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fs0와 fs1 모두 같은 A라는 path를 갖지만 값이 다른 경우, unification 결과는 None이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs0 = nltk.FeatStruct(A='a')\n",
    "fs1 = nltk.FeatStruct(A='b')\n",
    "fs2 = fs0.unify(fs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs0 = nltk.FeatStruct(\"\"\"[NAME=Lee, \n",
    "    ADDRESS=[NUMBER=74, STREET='rue Pascal'], \n",
    "    SPOUSE= [NAME=Kim, ADDRESS=[NUMBER=74,STREET='rue Pascal']]]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n"
     ]
    }
   ],
   "source": [
    "print(fs0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배우자의 주소에 Paris를 CITY의 value로 추가하고 싶다. 내 부인은 빠리지엥이니까."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(\"[SPOUSE = [ADDRESS = [CITY = Paris]]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [           [ CITY   = 'Paris'      ] ] ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n"
     ]
    }
   ],
   "source": [
    "print(fs1.unify(fs0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structure를 share하는 structure-sharing 버전의 경우에는 조금 다른 결과를 낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs2 = nltk.FeatStruct(\"\"\"[NAME=Lee, ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n",
    "    SPOUSE=[NAME=Kim, ADDRESS->(1)]]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n"
     ]
    }
   ],
   "source": [
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[               [ CITY   = 'Paris'      ] ]\n",
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n"
     ]
    }
   ],
   "source": [
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extending a Feature based Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Subcategorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 아래 표현식의 의미는?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - __VP -> IV__\n",
    "\n",
    " - __VP -> TV NP__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동사 타동사가 동사의 서로 다른 두 종류라는 것은 알고 있지만, 이러한 notation이 동사의 전반적인 특징을 설명해주지는 않는다. e.g. \"V의 범주에 포함되는 모든 단어는 시제를 나타낼 수 있다\"는 말을 할 수 없는데, 예를 들어, _walk_는 V가 아니라 IV의 범주에 들어가기 때문이다. 그렇다면 어떻게 V notation을 버리지 않고 IV와 TV를 모두 포함할 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SBar : 아마도 that절을 의미하는 듯 -> Comp S\n",
    "- Comp -> 'that'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Put the book on the table__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Heads Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 subcategorization를 통해 동사의 '특성'을 좀 더 일반화할 수 있었다. 또 다른 종류의 특성은 다음과 같다: V는 VP의 head, N은 NP의 head, 그리고 A는 AP의 head인 것처럼 _명사절은 명사가 이끄는 절, 동사절은 동사가 이끄는 절과 같은 특성_이다. 그렇다고 모든 절이 head가 있는 것은 아니지만 문법적으로 부모/자식의 관계를 밝혀서 표기하고 싶다. 지금은 V와 VP는 단지 atomic symbol에 불과하지만, 이 두 symbol을 feature를 이용해 연관짓는 방법을 찾아낼 것이다. 우리는 방법을 찾을 것이다. 늘 그래왔듯."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고 블로그 포스팅](http://blog.naver.com/frost9001/220665950655)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__X-bar Syntax__(영어학 관련 이론으로, PS Rule의 오류를 보완하기 위해 Chomsky가 채택하였다고 함)에서는 이 이슈에 대해 __phrasal level(절 수준)__의 표기를 빼는 방향으로 접근한다. \n",
    "\n",
    "예를 들어, N이 어휘 수준이라면 N'는 한 수준 위(전통적으로는 Nom으로 표기)를, N''는 phrasal level(전통적으로는 NP로 표기)에 해당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(34)에서 N은 structure의 head이고, N'과 N''는 __N의 (phrasal) projections__이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X-bar syntax의 주요 주장은 모든 구성요소가 구조적 유사성을 공유한다는 것이다. X 대신 N, V, A, P를 대입하여 사용하면 head인 X의 직속하위 범주에 속하는 _complements(보어)_는 항상 head의 sibling의 관계에 위치한다. 반면 _adjuncts(부사)_는 중간 범주(intermediate category)인 X'의 sibling의 관계에 위치한다. \n",
    "\n",
    "(35)에서의 두 개의 부사, P''와 (34a)의 한 개의 보어, P''는 대조적임을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Auxiliary Verbs and Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 두 경우는 동사와 주어의 위치가 뒤바뀐 절을 포함한 예문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(37)\t\t\n",
    " - Do you like children?\n",
    " - Can Jody walk?\n",
    "\n",
    "\n",
    "(38)\t\t\n",
    " - Rarely do you see Kim.\n",
    " - Never have I seen this dog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(37)에서와 같이 도치구문의 맨 처음에 올 수 있는 동사를 __auxiliaries__라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - __S[+INV] -> V[+AUX] NP VP__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(43)\t\t\n",
    " - You like Jody.\n",
    " - *You like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(43)에서 알 수 있듯 like는 NP complement를 필요로 한다. 그러나 아래 (45)의 경우에는 like 뒤에 complement는 생략 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(45)\t\t\n",
    " - Kim knows who you like.\n",
    " - This music, you really like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, 적당한 __filler__ 문장에 있을 경우, 예를 들어 의문사 _who_, like의 complement가 생략 가능하다. 여기서 (45)의 문장의 경우 complement가 생략된 자리에 __gap__을 포함한다고 말하는 것이 일반적이고, underscore를 통해 표기한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(47)\n",
    " - Which card do you put __ into the slot?\n",
    " - Which slot do you put the card into __?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, gap은 filler에 의해 자격을 인정받았을 때 생길 수 있다. 역으로, filler 문장 어딘가에 적절한 gap이 있을 때 생길 수 있다. 즉, 서로 필요한 존재라는 의미인듯. 오 맞음. __dependency__라고 부르기도 한다. 그러나 filler와 gap 사이의 거리가 아주 멀어도 상관 없으므로 찾기가 어려울수도 있겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(50)\t\t\n",
    " - Who do you like __?\n",
    " - Who do you claim that you like __?\n",
    " - Who do you claim that Jody says that you like __?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 특징들을 총망라하여 __unbounded dependency construction__이라는 이름으로 불리게 된다; that is, a filler-gap dependency where there is no upper bound on the distance between filler and gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 문장들의 문제점이 있는데! 생략된 sub-constituent를 어디다 표기할 것인가? __slash categories__를 이용하여 Y/XP로 나타내면, XP 범주의 sub-constituent가 없는 Y 범주의 phrase라고 해석하면 된다. 예를 들어, S/NP는 NP가 없는 S라고 해석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/51.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "S[-INV] -> NP VP\n",
      "S[-INV]/?x -> NP VP/?x\n",
      "S[-INV] -> NP S/NP\n",
      "S[-INV] -> Adv[+NEG] S[+INV]\n",
      "S[+INV] -> V[+AUX] NP VP\n",
      "S[+INV]/?x -> V[+AUX] NP VP/?x\n",
      "SBar -> Comp S[-INV]\n",
      "SBar/?x -> Comp S[-INV]/?x\n",
      "VP -> V[SUBCAT=intrans, -AUX]\n",
      "VP -> V[SUBCAT=trans, -AUX] NP\n",
      "VP/?x -> V[SUBCAT=trans, -AUX] NP/?x\n",
      "VP -> V[SUBCAT=clause, -AUX] SBar\n",
      "VP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\n",
      "VP -> V[+AUX] VP\n",
      "VP/?x -> V[+AUX] VP/?x\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "V[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\n",
      "V[SUBCAT=trans, -AUX] -> 'see' | 'like'\n",
      "V[SUBCAT=clause, -AUX] -> 'say' | 'claim'\n",
      "V[+AUX] -> 'do' | 'can'\n",
      "NP[-WH] -> 'you' | 'cats'\n",
      "NP[+WH] -> 'who'\n",
      "Adv[+NEG] -> 'rarely' | 'never'\n",
      "NP/NP ->\n",
      "Comp -> 'that'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (NP[+WH] who)\n",
      "  (S[+INV]/NP[]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[]/NP[]\n",
      "      (V[-AUX, SUBCAT='clause'] claim)\n",
      "      (SBar[]/NP[]\n",
      "        (Comp[] that)\n",
      "        (S[-INV]/NP[]\n",
      "          (NP[-WH] you)\n",
      "          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'who do you claim that you like'.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/feat1.fcfg')\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/52.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (NP[-WH] you)\n",
      "  (VP[]\n",
      "    (V[-AUX, SUBCAT='clause'] claim)\n",
      "    (SBar[]\n",
      "      (Comp[] that)\n",
      "      (S[-INV]\n",
      "        (NP[-WH] you)\n",
      "        (VP[] (V[-AUX, SUBCAT='trans'] like) (NP[-WH] cats))))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'you claim that you like cats'.split()\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (Adv[+NEG] rarely)\n",
      "  (S[+INV]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[] (V[-AUX, SUBCAT='intrans'] sing))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'rarely do you sing'.split()\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Case and Gender in German(생략)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The traditional categories of context-free grammar are atomic symbols. An important motivation for feature structures is to capture fine-grained distinctions that would otherwise require a massive multiplication of atomic categories.\n",
    " - By using variables over feature values, we can express constraints in grammar productions that allow the realization of different feature specifications to be inter-dependent.\n",
    " - Typically we specify fixed values of features at the lexical level and constrain the values of features in phrases to unify with the corresponding values in their children.\n",
    " - Feature values are either atomic or complex. A particular sub-case of atomic value is the Boolean value, represented by convention as [+/- f].\n",
    " - Two features can share a value (either atomic or complex). Structures with shared values are said to be re-entrant. Shared values are represented by numerical indexes (or tags) in AVMs.\n",
    " - A path in a feature structure is a tuple of features corresponding to the labels on a sequence of arcs from the root of the graph representation.\n",
    " - Two paths are equivalent if they share a value.\n",
    " - Feature structures are partially ordered by subsumption. FS0 subsumes FS1 when all the information contained in FS0 is also present in FS1.\n",
    " - The unification of two structures FS0 and FS1, if successful, is the feature structure FS2 that contains the combined information of both FS0 and FS1.\n",
    " - If unification adds information to a path π in FS, then it also adds information to every path π' equivalent to π.\n",
    " - We can use feature structures to build succinct analyses of a wide variety of linguistic phenomena, including verb subcategorization, inversion constructions, unbounded dependency constructions and case government."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
